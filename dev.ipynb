{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama.model import Transformer, ModelArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooks don't allow me to properly alter the positional embedding, as the positional embedding is not a separate module. Instead, I will alter the positional embedding directly in the implementation, which is why I have to fork llama's repo. Modifying it in the instantiation makes it non-general, same for modifying it in the rotational embedding application. So I will alter the representation directly in the forward, and make it dependent on an argument.\n",
    "\n",
    "Open question: How do I want to alter the rotary positional embedding?\n",
    "\n",
    "Then, I will make a new version of Llama from llama.generation called AlteredLlama, allowing to load llama from a file using an AlteredTransformer is created instead. I will likely need to overwrite all methods in order to pass the right arguments to the forward method of AlteredTransformer. This is really inconvenient.\n",
    "\n",
    "Do I have a cleaner way of doing it?\n",
    "\n",
    "I could instead create a mode for the AlteredTransformer that fixes whether the contexts must be reset or not, and using which indices. In this case, I still need to create AlteredLlama, but I can just override build for the loading as AlteredTransformer and define a method for AlteredLlama to switch the mode in the transformer. All generation code remains the same. For additional simplicity, I can create super-methods for each generation method where I first properly call the mode switch according to the prompt, then the desired generation method. I like that, I'll do this latter version.\n",
    "\n",
    "Note: An advantage of my implementation is that it can be used on top of the llama library, instead of requiring manipulations inside the library code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlteredTransformer(Transformer):\n",
    "    def  __init__(self, params: ModelArgs):\n",
    "        super().__init__(params)\n",
    "        self.alteration_mode = None\n",
    "        self.alteration_kwargs = dict()\n",
    "\n",
    "    def switch_mode(self, mode:list=None, **kwargs):\n",
    "        '''\n",
    "        Switch the model's alteration mode.\n",
    "        \n",
    "        @param mode: Which alteration mode to use. Valid values are {None, \"median\", \"reset\"}.\n",
    "        @param kwargs: Any kwargs needed for the specified alteration mode.\n",
    "\n",
    "        @returns None\n",
    "        '''\n",
    "        assert mode is None or mode in {\"median\", \"reset\"}, \"Invalid mode provided\"\n",
    "        # Note: I don't think it makes any sense to implement zero-patching to replace rotary positional embedding. But I can always add it later if we want to try it as well.\n",
    "\n",
    "        error_msg = \"Provide {} argument for mode {}.\"\n",
    "\n",
    "        # Asserting the validity of additional arguments for each mode\n",
    "        if mode in {\"median\", \"reset\"}:\n",
    "            indices = kwargs.get(\"indices\", None)\n",
    "            assert indices is not None, error_msg.format(\"indices\", mode)\n",
    "\n",
    "            if mode == \"reset\":\n",
    "                assert isinstance(indices, list) and (len(indices) > 1), \"Indices must be a list of at least two elements.\"\n",
    "            if mode == \"median\":\n",
    "                assert isinstance(indices, tuple) and (len(indices) == 2), \"Indices must be a tuple of two elements.\"\n",
    "            \n",
    "            previous = 0\n",
    "            for i in indices:\n",
    "                assert isinstance(i, int) and i >= 0, \"Indices provided must be non-negative integers\"\n",
    "                assert i > previous, \"Each index must be greater than the previous one.\"\n",
    "                previous = i\n",
    "        \n",
    "        self.alteration_mode = mode\n",
    "        self.alteration_kwargs = kwargs\n",
    "\n",
    "    def alter_positional_embedding(self, freqs_cis):\n",
    "        '''\n",
    "        Alter the positional embedding using the approach specified in self.alteration_mode.\n",
    "\n",
    "        @param freqs_cis: Frequencies to alter.\n",
    "\n",
    "        @returns torch.Tensor of the same shape as freqs_cis, which are the new positional embeddings to use.\n",
    "        '''\n",
    "        if self.alteration_mode is None:\n",
    "            return freqs_cis\n",
    "        if self.alteration_mode == \"median\":\n",
    "            raise NotImplementedError  # TODO\n",
    "        if self.alteration_mode == \"reset\":\n",
    "            raise NotImplementedError  # TODO\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):  # Let's overwrite the standard Transformer forward\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Input token indices.\n",
    "            start_pos (int): Starting position for attention caching.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits after applying the Transformer model.\n",
    "\n",
    "        \"\"\"\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        freqs_cis = self.alter_positional_embedding(freqs_cis)  # Modification compared to the forward of the superclass\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack([\n",
    "                torch.zeros((seqlen, start_pos), device=tokens.device),\n",
    "                mask\n",
    "            ]).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
